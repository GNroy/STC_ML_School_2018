{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of the solution is to use [VGGish](https://github.com/tensorflow/models/tree/1b728d473949c27ad93b90a16e2585ede407ad2f/research/audioset) preprocessing along with pre-trained convolution [layers](https://drive.google.com/open?id=16JrWEedwaZFVZYvn1woPKCuWx85Ghzkp).<br>\n",
    "There was two problems to handle:<br>\n",
    "[AudioSet](https://research.google.com/audioset/) contains no background and all data have a fixed length (10 sec).<br>\n",
    "So the first step here is to separate the background. It is achieved by Catboost on 4 simple features: mean, std, min, max.\n",
    "The second step is to handle too short files. It appears that the most of the files of less than 1 sec length are belong to a single class. Thus, we can check and predict them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wave\n",
    "import keras\n",
    "import sklearn\n",
    "import warnings\n",
    "import vggish_input\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import catboost as cb\n",
    "from vggish import VGGish\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.stats import mode as Mode\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 8\n",
    "NUM_CLASSES_NET = NUM_CLASSES - 1\n",
    "NUM_EPOCH = 3\n",
    "NUM_NETS = 5\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>background_0001.wav</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background_0001_time_stretch_0.wav</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background_0001_time_stretch_1.wav</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background_0001_time_stretch_10.wav</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>background_0001_time_stretch_11.wav</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     class\n",
       "name                                      \n",
       "background_0001.wav                      0\n",
       "background_0001_time_stretch_0.wav       0\n",
       "background_0001_time_stretch_1.wav       0\n",
       "background_0001_time_stretch_10.wav      0\n",
       "background_0001_time_stretch_11.wav      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta = pd.read_csv('./meta/meta.txt', sep='\\t', header=None, names=['name', 'class'], usecols=[0, 4], index_col=0)\n",
    "le = LabelEncoder()\n",
    "meta['class'] = le.fit_transform(meta['class'])\n",
    "meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8083aa5a1bb4492a45497219231999a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11307), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aposteriori_prob = np.zeros(NUM_CLASSES, dtype=np.int)\n",
    "\n",
    "def prepare_data(df):\n",
    "    X_net, Y_net, X_filter, Y_filter = [], [], [], []\n",
    "    for index, row in tqdm_notebook(df.iterrows(), total=df.size):\n",
    "        wav = wave.open('./audio/' + index, mode=\"r\")\n",
    "        frames = np.frombuffer(wav.readframes(wav.getnframes()), dtype=np.int16)\n",
    "        X_filter += [[np.mean(frames), np.std(frames), np.min(frames), np.max(frames)]]\n",
    "        Y_filter += [0 if row['class'] == 0 else 1]\n",
    "        if row['class'] != 0:\n",
    "            _, _, rate, length, _, _ = wav.getparams()\n",
    "            if rate <= length:\n",
    "                examples = vggish_input.wavfile_to_examples('./audio/' + index)\n",
    "                X_net += [ex for ex in examples]\n",
    "                Y_net += [row['class'] - 1 for ex in examples]\n",
    "            else:\n",
    "                aposteriori_prob[row['class']] += 1\n",
    "    X_net = np.array(X_net)[..., None]\n",
    "    Y_net = np.array(Y_net)\n",
    "    return X_net, Y_net, X_filter, Y_filter\n",
    "\n",
    "X_train, Y_train, X_filter, Y_filter = prepare_data(meta)\n",
    "aposteriori_prob = aposteriori_prob / aposteriori_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_filter = cb.CatBoostClassifier(iterations=NUM_EPOCH, depth=4, learning_rate=0.1, loss_function='Logloss', custom_loss=['Accuracy'], random_seed=123, logging_level='Silent')\n",
    "background_filter.fit(X_filter, Y_filter, verbose=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stabilize result, 5 nets are trained and used for voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Net 0\n",
      "\n",
      "Epoch 1/3\n",
      " - 62s - loss: 0.0931 - acc: 0.9708\n",
      "Epoch 2/3\n",
      " - 61s - loss: 0.0130 - acc: 0.9960\n",
      "Epoch 3/3\n",
      " - 63s - loss: 0.0059 - acc: 0.9981\n",
      "\n",
      "Training Net 1\n",
      "\n",
      "Epoch 1/3\n",
      " - 61s - loss: 0.0976 - acc: 0.9691\n",
      "Epoch 2/3\n",
      " - 63s - loss: 0.0107 - acc: 0.9967\n",
      "Epoch 3/3\n",
      " - 63s - loss: 0.0065 - acc: 0.9982\n",
      "\n",
      "Training Net 2\n",
      "\n",
      "Epoch 1/3\n",
      " - 64s - loss: 0.0930 - acc: 0.9712\n",
      "Epoch 2/3\n",
      " - 63s - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 3/3\n",
      " - 63s - loss: 0.0060 - acc: 0.9982\n",
      "\n",
      "Training Net 3\n",
      "\n",
      "Epoch 1/3\n",
      " - 64s - loss: 0.0940 - acc: 0.9710\n",
      "Epoch 2/3\n",
      " - 62s - loss: 0.0124 - acc: 0.9962\n",
      "Epoch 3/3\n",
      " - 58s - loss: 0.0063 - acc: 0.9978\n",
      "\n",
      "Training Net 4\n",
      "\n",
      "Epoch 1/3\n",
      " - 60s - loss: 0.0953 - acc: 0.9706\n",
      "Epoch 2/3\n",
      " - 58s - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 3/3\n",
      " - 55s - loss: 0.0078 - acc: 0.9975\n"
     ]
    }
   ],
   "source": [
    "def get_model(model_bottom):\n",
    "    x = model_bottom.get_layer(name=\"conv4/conv4_2\").output\n",
    "    x = Flatten(name='flatten_')(x)\n",
    "    x = Dense(512, activation=keras.backend.relu, name='vggish_fc1/fc1_1')(x)\n",
    "    x = Dropout(0.25, name='dropout_1')(x)\n",
    "    x = Dense(512, activation=keras.backend.relu, name='vggish_fc1/fc1_2')(x)\n",
    "    x = Dropout(0.25, name='dropout_2')(x)\n",
    "    x = Dense(NUM_CLASSES_NET, activation=keras.backend.softmax, name='vggish_fc2')(x)\n",
    "    model = Model(model_bottom.input, x, name='my_VGGish')\n",
    "    model.compile(optimizer=Adam(amsgrad=True), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_v = VGGish(include_top=False)\n",
    "for layer in model_v.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "models = []\n",
    "for i in range(NUM_NETS):\n",
    "    print()\n",
    "    print('Training Net '+ str(i))\n",
    "    print()\n",
    "    model = get_model(model_v)\n",
    "    model.fit(X_train, Y_train, epochs=NUM_EPOCH, verbose=2, batch_size=64)\n",
    "    models += [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdir_test = pd.Series(dict((name, name.split('_')[0]) for name in os.listdir('./test')))\n",
    "listdir_test[listdir_test == 'knocking'] = 'knocking_door'\n",
    "listdir_test_short = pd.Series(le.transform(listdir_test[listdir_test != 'unknown']), index=listdir_test.index[listdir_test != 'unknown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models):\n",
    "    known, known_mask, pred, pred_proba = [], [], [], []\n",
    "    for name in tqdm_notebook(listdir_test.index, total=listdir_test.size):\n",
    "        if listdir_test[name] == 'unknown':\n",
    "            known_mask += [False]\n",
    "        else:\n",
    "            known += [listdir_test_short[name]]\n",
    "            known_mask += [True]\n",
    "        wav = wave.open('./test/' + name, mode=\"r\")\n",
    "        frames = np.frombuffer(wav.readframes(wav.getnframes()), dtype=np.int16)\n",
    "        filter_proba = background_filter.predict_proba([np.array([np.mean(frames), np.std(frames), np.min(frames), np.max(frames)])])\n",
    "        filter_ans = np.argmax(filter_proba)\n",
    "        if filter_ans == 0:\n",
    "            pred += [filter_ans]\n",
    "            pred_proba += [filter_proba[0][filter_ans]]\n",
    "        else:\n",
    "            _, _, rate, length, _, _ = wav.getparams()\n",
    "            if rate <= length:\n",
    "                example = vggish_input.wavfile_to_examples('./test/' + name)\n",
    "                ans, ans_proba = [], []\n",
    "                for model in models:\n",
    "                    pred_batch = model.predict(np.array(example[..., None]))\n",
    "                    pred_elem = np.zeros(NUM_CLASSES_NET)\n",
    "                    for i in pred_batch:\n",
    "                        for j in range(NUM_CLASSES_NET):\n",
    "                            pred_elem[j] += i[j] / len(pred_batch)\n",
    "                    ans += [np.argmax(pred_elem)]\n",
    "                    ans_proba += [pred_elem]\n",
    "                mode = Mode(ans)[0][0]\n",
    "                pred += [mode + 1]\n",
    "                pred_proba += [np.mean(ans_proba, axis=0)[mode]]\n",
    "            else:\n",
    "                pred += [np.argmax(aposteriori_prob)]\n",
    "                pred_proba += [aposteriori_prob[pred[-1]]]\n",
    "    pred = np.array(pred, dtype=np.int)\n",
    "    pred_proba = np.array(pred_proba)\n",
    "    known = np.array(known)\n",
    "    known_mask = np.array(known_mask)\n",
    "    return len(pred[known_mask][pred[known_mask] == known]) / len(known), pred, pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9919c3cffa974b7fa7c41ffa81c8ad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=610), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expected accuracy: 0.959830866807611\n"
     ]
    }
   ],
   "source": [
    "expected_accuracy, pred_class, pred_proba = predict(models)\n",
    "print('Expected accuracy: ' + str(expected_accuracy))\n",
    "\n",
    "with open('result.txt', 'w') as fw:\n",
    "    for i in range(listdir_test.size):\n",
    "        fw.write(listdir_test.index[i] + '\\t' + '{:.3f}'.format(pred_proba[i]) + '\\t' + le.inverse_transform(pred_class[i]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
